{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseball Pitch Prediction - Model Evaluation\n",
    "\n",
    "This notebook evaluates the performance of our trained models and analyzes their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "import shap\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model comparison results\n",
    "results = pd.read_csv('models/results/model_comparison.csv')\n",
    "\n",
    "# Display results table\n",
    "print(\"Model Performance Comparison:\")\n",
    "results.style.format({\n",
    "    'Accuracy': '{:.3f}',\n",
    "    'Precision': '{:.3f}',\n",
    "    'Recall': '{:.3f}',\n",
    "    'F1 Score': '{:.3f}',\n",
    "    'ROC AUC': '{:.3f}'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar plot comparing model metrics\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC']\n",
    "results_melted = pd.melt(results, id_vars=['Model'], value_vars=metrics, var_name='Metric', value_name='Score')\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.barplot(data=results_melted, x='Model', y='Score', hue='Metric')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display confusion matrices\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "models = ['logistic_regression', 'decision_tree', 'random_forest', 'xgboost']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "for i, model in enumerate(models):\n",
    "    img = mpimg.imread(f'models/results/{model}_confusion_matrix.png')\n",
    "    ax = axes[i//2, i%2]\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(model.replace('_', ' ').title())\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display feature importance plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "for i, model in enumerate(models):\n",
    "    img = mpimg.imread(f'models/results/{model}_feature_importance.png')\n",
    "    ax = axes[i//2, i%2]\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'{model.replace(\"_\", \" \").title()} Feature Importance')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SHAP Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data and XGBoost model for SHAP analysis\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Load the most recent XGBoost model\n",
    "xgb_files = glob.glob('models/xgboost_*.pkl')\n",
    "latest_xgb = max(xgb_files, key=os.path.getctime)\n",
    "with open(latest_xgb, 'rb') as f:\n",
    "    xgb_model = pickle.load(f)\n",
    "\n",
    "# Load test data\n",
    "data = pd.read_csv('data/processed/modeling_data_2021_to_2023.csv')\n",
    "X = data.drop('is_fastball', axis=1)\n",
    "\n",
    "# Calculate SHAP values\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "# Plot SHAP summary\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Predictions Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze predictions in different game situations\n",
    "def predict_situation(model, situation_dict):\n",
    "    # Create a DataFrame with the situation\n",
    "    situation_df = pd.DataFrame([situation_dict])\n",
    "    \n",
    "    # Make prediction\n",
    "    prob = model.predict_proba(situation_df)[0][1]\n",
    "    return prob\n",
    "\n",
    "# Test common situations\n",
    "situations = [\n",
    "    {'balls': 0, 'strikes': 0, 'outs_when_up': 0, 'inning': 1, 'pitcher_fb_pct': 0.6},\n",
    "    {'balls': 3, 'strikes': 2, 'outs_when_up': 2, 'inning': 9, 'pitcher_fb_pct': 0.6},\n",
    "    {'balls': 0, 'strikes': 2, 'outs_when_up': 1, 'inning': 5, 'pitcher_fb_pct': 0.6}\n",
    "]\n",
    "\n",
    "for situation in situations:\n",
    "    prob = predict_situation(xgb_model, situation)\n",
    "    print(f\"\\nSituation: {situation}\")\n",
    "    print(f\"Fastball probability: {prob:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Calibration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze how well calibrated the model probabilities are\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "y = data['is_fastball']\n",
    "y_pred = xgb_model.predict_proba(X)[:, 1]\n",
    "\n",
    "prob_true, prob_pred = calibration_curve(y, y_pred, n_bins=10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(prob_pred, prob_true, marker='o')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('True Probability')\n",
    "plt.title('Calibration Plot')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
